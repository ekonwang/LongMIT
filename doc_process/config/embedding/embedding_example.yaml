embedder: # no change
  embedder_name: bge_base15_embedder
  backbone_type: BGE
  # init_backbone: BAAI/bge-base-en-v1.5
  init_backbone: /cpfs01/shared/public/wangyikun/ckpt/embedding_model/bge-base-en-v1.5
  flashatt: false
  pool_type: cls
  peft_lora: false
  which_layer: -1
  max_length: 512
  task_prompt: false
  checkpoint_batch_size: -1
  mytryoshka_size: 768
  embedding_norm: false
  embedder_ckpt_path:
  reserved_layers:
  device: cuda
  encode_batch_size: 2048

data:
  domain: wiki # source domain of text corpus
  input_dir: assets/example_datasets # path of original text files
  doc_glob: "custom_text_corpus.jsonl" # matched format of text files
  embed_output_dir: /cpfs01/shared/public/wangyikun/dataset/1029-longmit-embeddings # temporary folder to save embedding files